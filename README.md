# Smart-chatbot-using-deep-learning-Attention-mechanism


For developing smart chatbot, I have used Google's Neural machine Translation(NMT) Model which is based on Sequence to Sequence(Seq2Seq) modeling with encoder-decoder architecture. This encoder-decoder is using Recurrent Neural Network with bi-directional LSTM (Long-Short-Term-Memory) cells with 512 units for each. For performance optimization, I applied Bahdanau's Attention Mechanism during training. This lesson will show you how to develop smart chatbot by using the skill of deep learning computation for an Assistant Conversational Agent (Chatbot).

Lesson Goals

This lesson is the continuation of big data natural language text processing where we took three dataset from Answers.Com, Yahoo Answers, Quora API. In this lesson, I will show you how to develop smart open-domain chatbot by using deep learning Attention mechanism.

Prerequests

if your system has GPU you can run this example on your system but if not you can run it in GCP.
Getting Started
Import modules:
