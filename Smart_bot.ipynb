{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBvojspHGWrc"
   },
   "source": [
    "<h1><Center> Smart open-domain Chatbot using deep learning  Attention mechanism </Center></h1>\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgEIq_RCHqNe"
   },
   "source": [
    "# Lesson Goals\r\n",
    "This lesson is the continuation of big data natural language text processing where we took three dataset from Answers.Com, Yahoo Answers, Quora API. In this lesson, I will show you how to develop smart open-domain chatbot by using deep learning Attention mechanism.\r\n",
    "# Prerequests\r\n",
    "\r\n",
    "*   if your system has GPU you can run this example on your system but if not you can run it in GCP.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8pz2aaCJZQa"
   },
   "source": [
    "# Getting Started\n",
    "Import modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VlFctlhgJdDS"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "my6BV0KtJkAS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_VzstOtJs-R"
   },
   "source": [
    "## Data Preparation:\n",
    "# Load up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "c6kgd7vyJ3wL"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    \"\"\"\n",
    "    Reads the data from the csv file.\n",
    "    Arguments:\n",
    "        path: a string.\n",
    "    Returns:\n",
    "        dataset: a Pandas Dataframe with two columns, namely: `Question` and `Answer`.\n",
    "        X: a NumPy array representing `Question` column.\n",
    "        Y: a NumPy array representing `Answer` column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # read the csv file into a pandas dataframe\n",
    "    dataset = pd.read_csv(path, usecols=['Question', 'Answer'])\n",
    "    # make sure all cell values are strings; because some of them \n",
    "    # only contain numbers, so they maybe mistaken with other types.\n",
    "    dataset = dataset.applymap(str)\n",
    "    # shuffle the rows of the dataframe and then reset the index\n",
    "    dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X = np.asarray(dataset['Question'])\n",
    "    Y = np.asarray(dataset['Answer'])\n",
    "    \n",
    "    X = np.apply_along_axis(lambda sen: '<start> '+ sen + ' <end>', 0, X)\n",
    "    Y = np.apply_along_axis(lambda sen: '<start> '+ sen + ' <end>', 0, Y)\n",
    "    \n",
    "    return dataset, X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bhxQkXbIW3NB",
    "outputId": "e9d83235-1411-4707-9fee-3de821130a07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'os' from '/usr/lib/python3.6/os.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OixKyY2piyJB",
    "outputId": "5055b38b-d0c2-4870-d4c2-872e8e466e42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function posix.getcwd>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ybteJilpW8mY",
    "outputId": "94286cc4-73aa-4963-c8b6-d40e52337c08"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JQwL1XO-X5Ke"
   },
   "outputs": [],
   "source": [
    "!mkdir path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLvoeGBDYFCI",
    "outputId": "7694edcf-028e-46b5-d776-5ccaa16e0ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/path\n"
     ]
    }
   ],
   "source": [
    "cd path/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "P0SdSQK4YI0-",
    "outputId": "dd236cbb-a781-4b80-f5be-e68729b37cbf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-983212af-e48f-439b-b130-0982ce84baae\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-983212af-e48f-439b-b130-0982ce84baae\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset.csv to dataset.csv\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "33Z0V9_2YsoM",
    "outputId": "1cd73020-37ae-40d2-aecc-d858452c602d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what do you call a car if you don ' t know its...</td>\n",
       "      <td>mitsuheshe .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how do you end a relationship fight ?</td>\n",
       "      <td>you break it up .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what do you call a nun in a wheelchair ?</td>\n",
       "      <td>virgin mobile ( im going to fucking hell . i s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what did the drifter say to the person he hit ?</td>\n",
       "      <td>rip my e - brake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i didn ' t know what happiness was until i got...</td>\n",
       "      <td>but by then it was too late .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question                                             Answer\n",
       "0  what do you call a car if you don ' t know its...                                       mitsuheshe .\n",
       "1              how do you end a relationship fight ?                                  you break it up .\n",
       "2           what do you call a nun in a wheelchair ?  virgin mobile ( im going to fucking hell . i s...\n",
       "3    what did the drifter say to the person he hit ?                                   rip my e - brake\n",
       "4  i didn ' t know what happiness was until i got...                      but by then it was too late ."
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_path = '/content/path/'\n",
    "qa_dataframe, X, Y = load_dataset(files_path + 'dataset.csv')\n",
    "qa_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ge5qWhphaYGP",
    "outputId": "8c7e53bc-b7f5-4c0b-e8c2-baa67b85f93d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of question-answer pairs in the dataset: 176980\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of question-answer pairs in the dataset: {len(qa_dataframe)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfue4BGiarvq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tAMWya1b6zN"
   },
   "source": [
    "Cache all hyperparameters into this dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jxRAiIxlb8El"
   },
   "outputs": [],
   "source": [
    "hyperparameters = dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10tosrvqcc_7"
   },
   "source": [
    "**Choose the size of vocabulary**:\n",
    "\n",
    "This is a hyperparameter that you can play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EpLvCpzMchWa"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eab_AKFsc5Q5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mq5XOsQd_-z"
   },
   "source": [
    "Now, let's tokenize the data, by converting each sentence (question or answer) to a sequence of integers which represent indices of their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fSLTe8EQeBPa"
   },
   "outputs": [],
   "source": [
    "def tokenize(sentences, vocab_size):\n",
    "  \n",
    "    \"\"\"\n",
    "    Using Tensorflow Tokenizer to turn each sentence into a sequence of integers \n",
    "    (each integer being the index of a token in a dictionary).\n",
    "    Arguments:\n",
    "      X: a list or a NumPy array of strings, where each element is a sentence.\n",
    "    Returns:\n",
    "      tensor: a NumPy ndarray, where each row represents the the sequence of\n",
    "              integers that maps the words of the equivalent sentence in \n",
    "              the `sentences` list to a their indices (for embbedings). shape=(batch_size, )\n",
    "      lang_tokenizer: a Tensorflow Tokenizer which have been fit on `sentences`.\n",
    "    \"\"\" \n",
    "\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, \n",
    "                                                           filters='')\n",
    "    lang_tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(sentences)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BMAkgHRZj1nE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sInSx-SkjKWe",
    "outputId": "cdd8f491-cad8-43a2-bd85-c3b9bb6683f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts[0]= <start> what do you call a car if you don ' t know its gender ? <end>\n",
      "tensor[0]= [   1   10   16    8   38    5  173   51    8   60    4   21   58  197\n",
      " 1466    7    2    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# all sentences (Questions & Answers)\n",
    "# we need to fit the tokenizer on all sentences; to create word_index and \n",
    "# index_word mapper dictionaries for the most frequent VOCAB_SIZE= 10,000 words.\n",
    "texts = np.concatenate((X, Y))\n",
    "# tokenize the data\n",
    "tensor, text_tokenizer = tokenize(texts, VOCAB_SIZE)\n",
    "\n",
    "print(f\"texts[0]= {texts[0]}\")\n",
    "print(f\"tensor[0]= {tensor[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmtkW2j6kIGw"
   },
   "source": [
    "As we see, each sentence has been converted to an array with indices that will be used to map words to their embedding vectors.\n",
    "\n",
    "Now, let's extract back X (for questions) and Y (for answers) arrays from tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8IwWWzIij6x1",
    "outputId": "26ea69b1-ddb5-4ff8-cf07-d216adc62201"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X (questions): (176980, 32)\n",
      "Shape of Y (answers): (176980, 32)\n"
     ]
    }
   ],
   "source": [
    "# extract questions and answers back from tensor\n",
    "X, Y = tensor[:len(X)], tensor[len(X):]\n",
    "\n",
    "print(f\"Shape of X (questions): {X.shape}\")\n",
    "print(f\"Shape of Y (answers): {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvTrZyQnk3jv"
   },
   "source": [
    "Get the maximum sequence length for both input and target tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "fhxA6nfKk5mt"
   },
   "outputs": [],
   "source": [
    "def max_seq_length(tensor):\n",
    "    \"\"\"\n",
    "    Get maximum sequence length in the corpus. And, make sure that all rows in\n",
    "    `tensor` has the same length.\n",
    "    Arguments:\n",
    "      tensor: a NumPy ndarray of shape (batch_size,) where each row represents\n",
    "      indices mapping to words in equivalent sentences.\n",
    "    Returns:\n",
    "      max_len: an integer representing maximum sequence length.\n",
    "    \"\"\"\n",
    "    batch_size = len(tensor)\n",
    "    lengths = [len(sentence) for sentence in tensor]\n",
    "    max_len = max(lengths)\n",
    "\n",
    "    # check if all rows in `tensor` has the same length (equal to max_len)\n",
    "    assert lengths == [max_len]*batch_size\n",
    "\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_lspQyaDlpnW",
    "outputId": "6b4885dd-d3a8-4e84-edec-4c6a351356cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length for input (questions) tensor: 32\n",
      "Maximum sequence length for target (answers) tensor: 32\n"
     ]
    }
   ],
   "source": [
    "# Get max_seq_length of input and target tensors\n",
    "max_length_inp, max_length_targ = max_seq_length(X), max_seq_length(Y)\n",
    "print(f\"Maximum sequence length for input (questions) tensor: {max_length_inp}\")\n",
    "print(f\"Maximum sequence length for target (answers) tensor: {max_length_targ}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqZSW-HxmOGr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9hiXIPFmTKy"
   },
   "source": [
    "Save the arrays as npy files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jEIj7r3JmUVU"
   },
   "outputs": [],
   "source": [
    "data_arrays_path = os.path.join(files_path, 'data_arrays')\n",
    "# create the folder if it does not exist\n",
    "if not os.path.exists(data_arrays_path):\n",
    "    os.makedirs(data_arrays_path)\n",
    "    \n",
    "np.save(os.path.join(data_arrays_path, 'X.npy'), X)\n",
    "np.save(os.path.join(data_arrays_path, 'Y.npy'), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "eqkuDXpdmsG5"
   },
   "outputs": [],
   "source": [
    "def read_glove_vectors(glove_file):\n",
    "    \"\"\"\n",
    "    This function reads GloVe vectors from .txt file and \n",
    "    returns a word to vector dictionary.\n",
    "    Arguments:\n",
    "      glove_file: a string path to GloVe word embeddings file.\n",
    "    Returns:\n",
    "      word_to_vec: a Python dictionary that maps words to their embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    \n",
    "    # open the file\n",
    "    with open(glove_file, 'r', encoding=\"utf-8\") as f:\n",
    "        \n",
    "        words = set()\n",
    "        word_to_vec = {}\n",
    "\n",
    "        # loop over the rows in the file\n",
    "        for line in f:\n",
    "            # read the line, strip it (remove leading and trailing spaces) and split it\n",
    "            line = line.strip().split()\n",
    "            # first item in the list 'line' is the word itself\n",
    "            curr_word = line[0]\n",
    "            # add the word to set of words\n",
    "            words.add(curr_word)\n",
    "            # add the words with its vector representation as a (key, value) pair to the dictionary\n",
    "            word_to_vec[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "\n",
    "    return word_to_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2ElvRTHoPWq"
   },
   "source": [
    "## Download GloVe Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLdQisZzoLhC",
    "outputId": "ae7d0831-46eb-4b83-9f45-42b7da1fdb05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-28 17:06:09--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2020-12-28 17:06:09--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2020-12-28 17:06:10--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  1.57MB/s    in 7m 1s   \n",
      "\n",
      "2020-12-28 17:13:14 (1.95 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkzYZnMFoWe5",
    "outputId": "151e19ed-5883-4e6e-b9e3-22ed76a6a0ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h7ujxU3jqABS",
    "outputId": "780e1588-f341-4ea5-c2d6-eb49430a25d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_arrays  glove.6B.100d.txt\tglove.6B.300d.txt  glove.6B.zip\n",
      "dataset.csv  glove.6B.200d.txt\tglove.6B.50d.txt\n",
      "/content/path\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "_7tMpvoaqOSt"
   },
   "outputs": [],
   "source": [
    "glove_file = \"/content/path/glove.6B.200d.txt\"\n",
    "word_to_vec = read_glove_vectors(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tBwaBBHqeZD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVh6S9nBqsOc"
   },
   "source": [
    "# The Model\n",
    "**Choosing Hyperparameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "jL0Sa5t3q1IY"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = X.shape[0]\n",
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE \n",
    "embedding_dim = 200\n",
    "units = 512 \n",
    "vocab_size = VOCAB_SIZE + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kZx_YyGgs1uj",
    "outputId": "4eef287f-cde2-4db8-905e-3dbb13b4f32c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer size: 176980, Batch size: 128, Steps per epoch: 1382\n",
      "Embedding size: 200, # of units: 512\n",
      "Vocab size: 10001\n"
     ]
    }
   ],
   "source": [
    "print(f\"Buffer size: {BUFFER_SIZE}, Batch size: {BATCH_SIZE}, Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Embedding size: {embedding_dim}, # of units: {units}\")\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "-X5NDv5ktT-B"
   },
   "outputs": [],
   "source": [
    "# cache these values into the hyperparameters dictionary\n",
    "hyperparameters['buffer_size'] = BUFFER_SIZE\n",
    "hyperparameters['batch_size'] = BATCH_SIZE\n",
    "hyperparameters['steps_per_epoch'] = steps_per_epoch\n",
    "hyperparameters['embedding_dim'] = embedding_dim\n",
    "hyperparameters['units'] = units\n",
    "hyperparameters['vocab_size'] = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "bT8XjR-kuc_N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCM6FDiRumX_"
   },
   "source": [
    "Create the embedding matrix for words in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "wkEhofmluo-Q"
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(words, word_to_vec, vocab_size, emb_dim):\n",
    "  \"\"\"\n",
    "  Returns an embeddings matrix for the words in the vocabulary.\n",
    "  Arguments:\n",
    "      words: a list of words.\n",
    "      word_to_vec: a dictionary that maps words to their embedding vectors.\n",
    "      vocab_size: an integer which represents the size of the vocabulary.\n",
    "      emb_dim: an integer which represents the dimension of word embeddings.\n",
    "  Returns:\n",
    "      embedding_matrix: a NumPy array with shape of (vocab_size, emb_dim).\n",
    "  \"\"\"\n",
    "\n",
    "  # create embedding matrix\n",
    "  embedding_matrix = np.zeros((vocab_size, emb_dim), dtype=np.float64)\n",
    "\n",
    "  # loop over the words in our vocabulary\n",
    "  for i, word in enumerate(words):\n",
    "    if word in word_to_vec.keys():\n",
    "      # if the current word is in glove vocab, get its glove vector.\n",
    "      embedding_matrix[i, :] = word_to_vec[word]\n",
    "    else:\n",
    "      # if the current word does not exist in the vocabulary, set its vector to zeros.\n",
    "      embedding_matrix[i, :] = np.zeros((emb_dim,), dtype=np.float64)\n",
    "\n",
    "  return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "HXxhqXqhwDGd"
   },
   "outputs": [],
   "source": [
    "# since Tensorflow tokenizer word_index still have all words even when \n",
    "# vocab_size is passed while defining the Tokenizer. \n",
    "# So, we need to grab the first 10,000 words.\n",
    "words = list(text_tokenizer.word_index.keys())[:vocab_size]\n",
    "embedding_matrix = create_embedding_matrix(words, word_to_vec, vocab_size, \n",
    "                                           embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DYiK-GDEwSD4",
    "outputId": "6de4867a-f79b-460f-c1c2-e2aa38da33b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (10001, 200)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "LbcZP8jkwYIQ"
   },
   "outputs": [],
   "source": [
    "# save the embedding matrix\n",
    "np.save(os.path.join(data_arrays_path, 'embedding_matrix.npy'), embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "aMAGfbLRwhD1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGh7vksnwul5"
   },
   "source": [
    "Create a tf.data dataset for X and Y with buffer size equal to BUFFER_SIZE and batch size equal to BATCH_SIZE, we will use this dataset to generate batches while training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "oOfB0cnJwvx3"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X, Y)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "3FeLjHWOxskF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HadyHRWvxz1G"
   },
   "source": [
    "Example of an input batch that the model will receive while iterating over dataset batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7WfYzk3Ox0sb",
    "outputId": "1681fb8d-47da-4740-c3d2-35eea2b9de9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([128, 32]), TensorShape([128, 32]))"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "AvC3I9MBx_Jr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GP0bcRIyIL1"
   },
   "source": [
    "# Model Architecture\n",
    "I'm going to use an encoder-decoder model with Bahdanau's attention, which has been described in detail in this paper:\n",
    "\n",
    "[Effective Approaches to Attention-based Neural Machine Translation](https://)\n",
    "\n",
    "Some properties of the model:\n",
    "\n",
    "The encoder will be a bidirectional encoder with 512 hidden units for each direction, resulting in 1024 cells.\n",
    "\n",
    "Both encoder & decoder will use LSTM as the cell.\n",
    "\n",
    "The decoder will be unidirectional with 1024 hidden units.\n",
    "\n",
    "**The Encoder**\n",
    "Define the bidirectional encoder architecture which will consist of the following:\n",
    "\n",
    "   1. An Embedding layer that will map input sentences to their embeddings. The vocabulary size is equal to VOCAB_SIZE=10,000 and the dimension of embedding is 200 (Note: although a power of 2 embedding size would be more suitable to speed up training time by increasing cache utilization during data movement, thus reducing bottlenecks, but GloVe embeddings do not come with a power of 2 embedding size).\n",
    "\n",
    "   2. A forward & backward (bidirectional) LSTM layer with 512 units for each.\n",
    "\n",
    "The input to the encoder has a shape of (batch_size, ), which is the input array of sentences (questions).\n",
    "\n",
    "The hidden state arrays of the encoder are all of the shape (batch_size, n_units).\n",
    "\n",
    "The output of the encoder has a shape of (batch_size, max_sequence_length, n_units x2) ;the hidden size of the output is equal to double the size of units since the encoder is bidirectional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "SdxXkt5zyo8e"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, enc_units, \n",
    "                 batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                                                   weights=[embedding_matrix], \n",
    "                                                   trainable=True)\n",
    "        self.bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Propogates the input `x` through the bidirectional encoder and returns\n",
    "        the outputs along with hidden states.\n",
    "        Arguments:\n",
    "            x: a tensor with shape (batch_size, max_seq_length)\n",
    "            hidden: a tuple or a list, with four tensors representing hidden\n",
    "                    and memory states for both the forward and backward LSTMs.\n",
    "                    Each of them having a shape of (batch_size, max_seq_length)\n",
    "        Returns:\n",
    "            output: a tensor representing the output of the encoder with a shape\n",
    "                    of (batch size, max_sequence length, units*2)\n",
    "            state_h: a tensor, with forward and backward hidden states of \n",
    "                     the encoder with a shape of (batch size, units*2)\n",
    "            state_c: a tensor, with forward and backward memory cell states of\n",
    "                     the encoder with a shape of (batch size, units*2)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        output, fstate_h, fstate_c, bstate_h, bstate_c = self.bi_lstm(x, initial_state = hidden)\n",
    "\n",
    "        state_h = tf.keras.layers.Concatenate()([fstate_h, bstate_h])\n",
    "        state_c = tf.keras.layers.Concatenate()([fstate_c, bstate_c])\n",
    "\n",
    "        return output, state_h, state_c\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        # forward_state_h forward hidden state output, backward_state_h backward hidden state output\n",
    "        # forward_state_c forward cell (memory) state output, backward_state_c backward cell (memory) state output\n",
    "        return (tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units)), \n",
    "                tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "P7Nguxho111f"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Inyjb2reZONk"
   },
   "source": [
    "Define the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "saSLZxRrZPik",
    "outputId": "86158968-f67d-4c09-ff28-e8163ea4747e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units*2) (128, 32, 1024)\n",
      "Encoder Hidden state shape: (batch size, units*2) (128, 1024)\n",
      "Encoder Memory state shape: (batch size, units*2) (128, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size, embedding_dim, embedding_matrix, units, \n",
    "                  BATCH_SIZE)\n",
    "\n",
    "# usage example\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "\n",
    "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
    "print (f'Encoder output shape: (batch size, sequence length, units*2) {sample_output.shape}')\n",
    "print (f'Encoder Hidden state shape: (batch size, units*2) {sample_h.shape}')\n",
    "print (f'Encoder Memory state shape: (batch size, units*2) {sample_c.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZLjE5w0qmoJ"
   },
   "source": [
    "Bahdanau Attention Mechanism\n",
    "The following diagram shows that each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence. The below picture and formulas are an example of attention mechanism from Effective Approaches to Attention-based Neural Machine Translation.\n",
    "To learn more about Bahdanau's attention, you can refer to this paper: [Neural Machine Translation by Jointly Learning to Align and Translate](https://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "-uqAKPwnqqx1"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units*2)\n",
    "        self.W2 = tf.keras.layers.Dense(units*2)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "XsfWisphs8LI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Sbg0HfKuEs5"
   },
   "source": [
    "Example of using Bahdanau's attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fAomKEoeuGQ4",
    "outputId": "63d24d85-8ae1-434a-8537-b845348af42f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units*2) (128, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (128, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_h, sample_output)\n",
    "\n",
    "print(f\"Attention result shape: (batch size, units*2) {attention_result.shape}\")\n",
    "print(f\"Attention weights shape: (batch_size, sequence_length, 1) {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "x9dIEFkfuRlc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKZBrXBZvtGG"
   },
   "source": [
    "# The Decoder\n",
    "Define the decoder architecture which will consist of the following:\n",
    "\n",
    "  1. Bahdanau's attention which is applied to the output of the encoder to get the context which will be passed, along with the embeddings of the decoder input, to the LSTM layer of the decoder.\n",
    "\n",
    "  2. An Embedding layer with a vocabulary size of VOCAB_SIZE=10,000 words and an embedding dimension of 200. The embedding takes the decoder input which is of the shape (batch_size, ) and outputs a tensor of the shape (batch_size, 1, embedding_dim).\n",
    "\n",
    "   3. A forward (unidirectional) LSTM layer with 1024 units for each.\n",
    "\n",
    "The shape of the decoder's output is (batch_size, vocab_size) and the decoder hidden state size is of the shape (batch_size, units x2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "cEE9qNkbwCYR"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, dec_units, \n",
    "                 batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                                   weights=[embedding_matrix],\n",
    "                                                   trainable=True)\n",
    "        self.lstm = tf.keras.layers.LSTM(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        \"\"\"\n",
    "        Takes decoder input, hidden state and output of the encoder and\n",
    "        returns the decoder's predictions along with decoder hidden state and\n",
    "        attention weights.\n",
    "        Arguments:\n",
    "            x: a tensor with shape (batch_size, 1) which is the decoder input\n",
    "               at some timestep.\n",
    "            hidden: a tensor representing hidden state of the encoder with a\n",
    "                    shape of (batch size, units*2)\n",
    "            enc_output: a tensor representing the encoder's output with a shape\n",
    "                        of (batch size, sequence length, units*2)\n",
    "        Returns:\n",
    "            x: a tensor with shape (batch_size, vocab size) and it's decoder's\n",
    "               output.\n",
    "            state: a tensor that represents the hidden state of the decoder \n",
    "                   and has a shape of (batch_size, units*2)\n",
    "            attention_weights: a tensor that represents attention weights and \n",
    "                               has a shape of (batch_size, sequence_length, 1)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the LSTM\n",
    "        output, state_h, state_c = self.lstm(x)\n",
    "\n",
    "        state = state_h\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "BG35BcgowQyQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAzhl9JW5FUF"
   },
   "source": [
    "Define the decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zi4sRGFI5GcR",
    "outputId": "af89cb85-f5ed-4981-adc2-969500f10954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (128, 10001)\n",
      "Decoder hidden state shape: (batch_size, units*2) (128, 1024)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_size, embedding_dim, embedding_matrix, units*2, \n",
    "                  BATCH_SIZE)\n",
    "\n",
    "# usage example\n",
    "sample_decoder_output, dec_h, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_h, sample_output)\n",
    "\n",
    "print(f'Decoder output shape: (batch_size, vocab size) {sample_decoder_output.shape}')\n",
    "print(f'Decoder hidden state shape: (batch_size, units*2) {dec_h.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "rR1o5CFa5LLO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95qhVlR25YiD"
   },
   "source": [
    "# Defining Optimizer, Loss Function and Metric\n",
    " Define the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "4tkdI4ko5epc"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, \n",
    "                                                            reduction='none')\n",
    "\n",
    "def compute_loss(real, pred):\n",
    "    \"\"\"\n",
    "    This function returns the loss for model's predictions on a batch \n",
    "    of data in comparison with the real outputs at a timestep.\n",
    "    Arguments:\n",
    "        real: real output, a Tensorflow tensor with a shape \n",
    "              of: (batch_size, max_seq_length)\n",
    "        pred: model's predictions at a certain timestep, a Tensorflow tensor \n",
    "              with a shape of: (batch_size, max_seq_length)\n",
    "    Returns:\n",
    "        A Tensorflow tensor with the loss.\n",
    "    \"\"\"\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "NBiady7P56EC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7Jxosdx5_bN"
   },
   "source": [
    "Define the perplexity metric:\n",
    "\n",
    "**Note**: I was going to incorporate BLEU score as another metric, but to compute BLEU score on a batch of data, it turns out that we need access the data to count the number of n-grams. In order to do that, we can't use Tensorflow's AutoGraph feature tf.function. Anyways, I tried it and using tf.function was around 2.5 times faster than dropping this feature only to compute BLEU score. In case you want to use BLEU score, you can use nmt's open source implementation which you can find [here](https://), or you can use NLTK's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "BLo6Sikl6ELS"
   },
   "outputs": [],
   "source": [
    "def compute_perplexity(real, pred):\n",
    "    \"\"\"\n",
    "    This function returns the perplexity for model's predictions on a batch \n",
    "    of data in comparison with the real outputs at a timestep.\n",
    "    Arguments:\n",
    "        real: real output, a Tensorflow tensor with a shape \n",
    "              of: (batch_size, max_seq_length)\n",
    "        pred: model's predictions at a certain timestep, a Tensorflow tensor \n",
    "              with a shape of: (batch_size, max_seq_length)\n",
    "    Returns:\n",
    "        A Tensorflow tensor with the perplexity.\n",
    "    \"\"\"\n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.cast(tf.pow(math.e, tf.keras.backend.mean(loss_, axis=-1)), \n",
    "                   dtype=tf.keras.backend.floatx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "ExRXFbR56g6_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5y_Cijoc7BrL"
   },
   "source": [
    "# Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "EQPRj8j77D4x"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = '/content/path/training_checkpoints'\n",
    "\n",
    "# create the folder if it does not exist\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "sHIopPWj7fpM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxkRQDXz7pYe"
   },
   "source": [
    "# TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "lmJ4GSiu7s_V"
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "I-12nHE57xTu"
   },
   "outputs": [],
   "source": [
    "log_file_name =f\"/content/path/logs/metrics_{int(time.time())}\"\n",
    "\n",
    "# create the folder if it does not exist\n",
    "if not os.path.exists(log_file_name):\n",
    "    os.makedirs(log_file_name)\n",
    "    \n",
    "summary_writer = tf.summary.create_file_writer(log_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "c68Q00cF8Efg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XE9zfxW8MA4"
   },
   "source": [
    "# Define training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "UUOSmhOM8PLo"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    \"\"\"\n",
    "    This function performs a training step for the model on a batch of data.\n",
    "    Arguments:\n",
    "        inp: a tensor, the input to the encoder network which is a batch of \n",
    "              vectors of integers (indices of words) for sentences (questions) \n",
    "              with a shape of (batch_size, encoder_max_seq_len)\n",
    "        targ: a tensor, the real output that the decoder will use to learn \n",
    "              using teacher forcing. It has a shape same as `inp`\n",
    "              which is (batch_size, encoder_max_seq_len)\n",
    "        enc_hidden: a tuple of four tensors, the initial hidden states for the\n",
    "              encoder network, each of the shape (batch_size, n_units*2)\n",
    "    Returns:\n",
    "        batch_loss: loss for the given batch.\n",
    "        batch_acc: accuracy for the given batch.\n",
    "        batch_bleu: bleu score for the given batch.\n",
    "        batch_ppl: perplexity for the given batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = 0\n",
    "    ppl = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Run the input through the encoder and get back the encoder output\n",
    "        # and the hidden states of the encoder.\n",
    "        enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
    "        \n",
    "        # Set the hidden state of the encoder to be the initial hidden state\n",
    "        # of the decoder.\n",
    "        dec_hidden = enc_h\n",
    "\n",
    "        # Define the decoder input which is basically the '<start>' token,\n",
    "        # for every sentence in the batch.\n",
    "        dec_input = tf.expand_dims([text_tokenizer.word_index['<start>']] \n",
    "                                   * BATCH_SIZE, 1)\n",
    "        \n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        # looping over timesteps\n",
    "        for t in range(1, targ.shape[1]):\n",
    "\n",
    "            # Pass encoder output to the decoder along with the decoder\n",
    "            # input and initial hidden state and get back the predictions\n",
    "            # for this batch at the current timestep with the hidden state \n",
    "            # of the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, \n",
    "                                                 dec_hidden, \n",
    "                                                 enc_output)\n",
    "            # predictions shape: (batch_size, decoder_vocab_size)\n",
    "            # dec_hidden shape: (batch_size, units*2)\n",
    "            # attention weights (3rd output that has been discarded) \n",
    "            # shape: (batch_size, decoder_max_seq_len, 1)\n",
    "\n",
    "            # compute the loss\n",
    "            loss += compute_loss(targ[:, t], predictions)\n",
    "\n",
    "            # compute the perplexity\n",
    "            ppl += compute_perplexity(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    # compute the loss for the batch\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    \n",
    "    # compute the perplexity for the batch\n",
    "    batch_ppl = (ppl / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    # get gradients\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss, batch_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "guTsnoOF8aBq"
   },
   "outputs": [],
   "source": [
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return f\"{h}:{m}:{round(s,1)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "ir_y3gxj_QZM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNW0yWb9_Wal"
   },
   "source": [
    "## Training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FwvI1LJT_YWE",
    "outputId": "9d54da1f-bfb7-4569-f063-5b752a785591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'buffer_size': 176980, 'batch_size': 128, 'steps_per_epoch': 1382, 'embedding_dim': 200, 'units': 512, 'vocab_size': 10001}\n"
     ]
    }
   ],
   "source": [
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObaDUvlZ_eP4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SARxgmL_6Sv"
   },
   "source": [
    "Let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sJ42PyUl_7p8",
    "outputId": "e45b5516-b9d1-48d0-d7b5-3f33e69f1d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - batch: 0/1382 - loss: 3.0469369888305664 - ppl: 967.1176147460938\n",
      "Epoch 1/10 - batch: 400/1382 - loss: 1.3774396181106567 - ppl: 18.1285400390625\n",
      "Epoch 1/10 - batch: 800/1382 - loss: 1.372939109802246 - ppl: 13.611625671386719\n",
      "Epoch 1/10 - batch: 1200/1382 - loss: 1.372182011604309 - ppl: 11.62140941619873\n",
      "Epoch 1/10 - loss: 1.4264665842056274 - ppl: 21.072736740112305\n",
      "Time taken for epoch (1): 1093.9495713710785 sec\n",
      "\n",
      "Epoch 2/10 - batch: 0/1382 - loss: 1.2829258441925049 - ppl: 9.259562492370605\n",
      "Epoch 2/10 - batch: 400/1382 - loss: 1.1151574850082397 - ppl: 9.946002006530762\n",
      "Epoch 2/10 - batch: 800/1382 - loss: 1.1067540645599365 - ppl: 8.797042846679688\n",
      "Epoch 2/10 - batch: 1200/1382 - loss: 1.1032754182815552 - ppl: 6.854518890380859\n",
      "Epoch 2/10 - loss: 1.1896915435791016 - ppl: 8.73717975616455\n",
      "Time taken for epoch (2): 1063.7512044906616 sec\n",
      "\n",
      "Epoch 3/10 - batch: 0/1382 - loss: 1.0923715829849243 - ppl: 5.646231651306152\n",
      "Epoch 3/10 - batch: 400/1382 - loss: 1.0370644330978394 - ppl: 6.776115894317627\n",
      "Epoch 3/10 - batch: 800/1382 - loss: 1.0456619262695312 - ppl: 6.476316452026367\n",
      "Epoch 3/10 - batch: 1200/1382 - loss: 1.0749843120574951 - ppl: 6.259608268737793\n",
      "Epoch 3/10 - loss: 1.0735533237457275 - ppl: 6.487866401672363\n",
      "Time taken for epoch (3): 1064.3578884601593 sec\n",
      "\n",
      "Epoch 4/10 - batch: 0/1382 - loss: 1.023870825767517 - ppl: 5.408150672912598\n",
      "Epoch 4/10 - batch: 400/1382 - loss: 0.9861414432525635 - ppl: 5.178683757781982\n",
      "Epoch 4/10 - batch: 800/1382 - loss: 0.8857904672622681 - ppl: 4.135000228881836\n",
      "Epoch 4/10 - batch: 1200/1382 - loss: 1.012370228767395 - ppl: 5.196199893951416\n",
      "Epoch 4/10 - loss: 0.9772443175315857 - ppl: 5.120789527893066\n",
      "Time taken for epoch (4): 1064.8860213756561 sec\n",
      "\n",
      "Epoch 5/10 - batch: 0/1382 - loss: 0.8830953240394592 - ppl: 4.266592502593994\n",
      "Epoch 5/10 - batch: 400/1382 - loss: 0.8393645286560059 - ppl: 4.052507400512695\n",
      "Epoch 5/10 - batch: 800/1382 - loss: 0.7930567264556885 - ppl: 3.3660497665405273\n",
      "Epoch 5/10 - batch: 1200/1382 - loss: 0.9604834318161011 - ppl: 4.672984600067139\n",
      "Epoch 5/10 - loss: 0.8881324529647827 - ppl: 4.156533718109131\n",
      "Time taken for epoch (5): 1066.0626738071442 sec\n",
      "\n",
      "Epoch 6/10 - batch: 0/1382 - loss: 0.7803549766540527 - ppl: 3.1223552227020264\n",
      "Epoch 6/10 - batch: 400/1382 - loss: 0.6711603403091431 - ppl: 2.7564454078674316\n",
      "Epoch 6/10 - batch: 800/1382 - loss: 0.7741417288780212 - ppl: 3.420640230178833\n",
      "Epoch 6/10 - batch: 1200/1382 - loss: 0.8523339033126831 - ppl: 3.6948719024658203\n",
      "Epoch 6/10 - loss: 0.8034955263137817 - ppl: 3.438868761062622\n",
      "Time taken for epoch (6): 1065.8163585662842 sec\n",
      "\n",
      "Epoch 7/10 - batch: 0/1382 - loss: 0.7321226596832275 - ppl: 2.720815896987915\n",
      "Epoch 7/10 - batch: 400/1382 - loss: 0.7048544883728027 - ppl: 2.871669292449951\n",
      "Epoch 7/10 - batch: 800/1382 - loss: 0.7153537273406982 - ppl: 2.8289523124694824\n",
      "Epoch 7/10 - batch: 1200/1382 - loss: 0.8010708093643188 - ppl: 3.0171585083007812\n",
      "Epoch 7/10 - loss: 0.7244855165481567 - ppl: 2.903376340866089\n",
      "Time taken for epoch (7): 1065.8014180660248 sec\n",
      "\n",
      "Epoch 8/10 - batch: 0/1382 - loss: 0.5275856256484985 - ppl: 2.0478689670562744\n",
      "Epoch 8/10 - batch: 400/1382 - loss: 0.6158868074417114 - ppl: 2.307462215423584\n",
      "Epoch 8/10 - batch: 800/1382 - loss: 0.6939862370491028 - ppl: 2.6901912689208984\n",
      "Epoch 8/10 - batch: 1200/1382 - loss: 0.7197328209877014 - ppl: 2.7474617958068848\n",
      "Epoch 8/10 - loss: 0.6513014435768127 - ppl: 2.5013954639434814\n",
      "Time taken for epoch (8): 1066.370909690857 sec\n",
      "\n",
      "Epoch 9/10 - batch: 0/1382 - loss: 0.5303057432174683 - ppl: 1.9675402641296387\n",
      "Epoch 9/10 - batch: 400/1382 - loss: 0.5571733117103577 - ppl: 2.144164562225342\n",
      "Epoch 9/10 - batch: 800/1382 - loss: 0.5966367125511169 - ppl: 2.275474786758423\n",
      "Epoch 9/10 - batch: 1200/1382 - loss: 0.6924189329147339 - ppl: 2.5128331184387207\n",
      "Epoch 9/10 - loss: 0.5846563577651978 - ppl: 2.2001781463623047\n",
      "Time taken for epoch (9): 1066.120908498764 sec\n",
      "\n",
      "Epoch 10/10 - batch: 0/1382 - loss: 0.47342824935913086 - ppl: 1.8078056573867798\n",
      "Epoch 10/10 - batch: 400/1382 - loss: 0.5373736619949341 - ppl: 2.0681798458099365\n",
      "Epoch 10/10 - batch: 800/1382 - loss: 0.4942871332168579 - ppl: 1.861227035522461\n",
      "Epoch 10/10 - batch: 1200/1382 - loss: 0.5555866360664368 - ppl: 2.1047890186309814\n",
      "Epoch 10/10 - loss: 0.5239253640174866 - ppl: 1.9709444046020508\n",
      "Time taken for epoch (10): 1065.1697010993958 sec\n",
      "\n",
      "Elapsed time: 2:58:2.3\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# to cache loss and perlexity over epochs\n",
    "cache = dict({'train_loss': [], 'train_ppl':[]})\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    for epoch in range(EPOCHS):\n",
    "        start_epoch = time.time()\n",
    "\n",
    "        # Initialize encoder hidden state\n",
    "        enc_hidden = encoder.initialize_hidden_state()\n",
    "\n",
    "        total_loss = 0\n",
    "        total_ppl = 0\n",
    "        \n",
    "        # Training the model using the training data\n",
    "        for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "            \n",
    "            # Train the model on current batch\n",
    "            (batch_loss, batch_ppl) = train_step(inp, targ, enc_hidden)\n",
    "\n",
    "            total_loss += batch_loss\n",
    "            total_ppl += batch_ppl  \n",
    "\n",
    "            # print loss and perplexity for current batch\n",
    "            if batch % 400 == 0:\n",
    "              print(f\"Epoch {epoch + 1}/{EPOCHS} - \"\n",
    "                      f\"batch: {batch}/{steps_per_epoch} - \"\n",
    "                      f\"loss: {batch_loss.numpy()} - ppl: {batch_ppl}\")\n",
    "        \n",
    "        # compute batch loss and perplexity\n",
    "        total_loss = total_loss / steps_per_epoch\n",
    "        total_ppl = total_ppl / steps_per_epoch     \n",
    "\n",
    "        # Log loss and perplexity to TensorBoard for current epoch\n",
    "        with summary_writer.as_default():\n",
    "          tf.summary.scalar('training_loss', total_loss, step=epoch)\n",
    "          tf.summary.scalar('training_perplexity', total_ppl, step=epoch)\n",
    "\n",
    "        # Save (checkpoint) the model every 15 epochs\n",
    "        if ((epoch+1) > 1) and ((epoch+1) % 15 == 0):\n",
    "          checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "          print(f'Saved checkpoint for epoch {epoch+1}/{EPOCHS} to: {checkpoint_prefix}')\n",
    "        \n",
    "\n",
    "        # cache the loss and perplexity for current epoch\n",
    "        cache['train_loss'].append(total_loss)\n",
    "        cache['train_ppl'].append(total_ppl)\n",
    "\n",
    "        # print loss and perplexity for current epoch\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS} - loss: {total_loss} - \"\n",
    "              f\"ppl: {total_ppl}\")\n",
    "        \n",
    "        print(f\"Time taken for epoch ({epoch + 1}): \"\n",
    "              f\"{time.time() - start_epoch} sec\\n\")\n",
    "\n",
    "execution_time = (time.time() - start_time)\n",
    "print(f'Elapsed time: {hms_string(execution_time)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWcQLRstauOw"
   },
   "source": [
    "Plot metrics over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "Fe-lcqrlAYcE"
   },
   "outputs": [],
   "source": [
    "def plot_metrics(cache):\n",
    "    \"\"\"\n",
    "    Plots loss and perplexity over epochs:\n",
    "    Arguemnts:\n",
    "        cache: a dictionary that contains values over epochs\n",
    "               for both loss and perplexity.\n",
    "    \"\"\"\n",
    "    \n",
    "    # since, items of cache['train_loss'] and cache['train_ppl']\n",
    "    # are tensors, so let's extract values from these tensors.\n",
    "    loss = [tensor.numpy() for tensor in cache['train_loss']]\n",
    "    ppl = [tensor.numpy() for tensor in cache['train_ppl']]\n",
    "    \n",
    "    # Plot the loss\n",
    "    plt.figure()\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the perplexity\n",
    "    plt.figure()\n",
    "    plt.plot(ppl, label='Training Accuracy')\n",
    "    plt.title('Perplexity')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "48tczVrMbQFw",
    "outputId": "55c8821a-433d-42fa-c75a-323177d0777f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGWHf6eWbsVC"
   },
   "source": [
    "# Inference Mode\n",
    "Now, it's time to try the model and ask him a few questins :)\n",
    "\n",
    "Helper function: to clean the text the user has entered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "F25u2_84bxbj"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    This method takes a string, applies different text preprocessing \n",
    "    (characters replacement, removal of unwanted characters, removal of extra \n",
    "    whitespaces) operations and returns a string.\n",
    "    Arguments:\n",
    "      text: a string.\n",
    "    Returns:\n",
    "      a cleaned version of text.\n",
    "    \"\"\"\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # REPLACEMENT\n",
    "    text = re.sub('\\\"', '\\'', text)\n",
    "    text = re.sub(\"“\", '\\'', text)\n",
    "    text = re.sub(\"”\", '\\'', text)\n",
    "    text = re.sub('’', '\\'', text)\n",
    "    text = re.sub('\\[','(', text)\n",
    "    text = re.sub('\\]',')', text)\n",
    "    text = re.sub('\\{','(', text)\n",
    "    text = re.sub('\\}',')', text)\n",
    "    text = re.sub(\"([?.!,:;'?!+\\-*/=%$@&()])\", r\" \\1 \", text)\n",
    "\n",
    "    pattern = re.compile('[^a-zA-Z0-9_\\.\\,\\:\\;\\'\\?\\!\\+\\-\\*\\/\\=\\%\\$\\@\\&\\(\\)]')\n",
    "    # remove unwanted characters\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    \n",
    "    # lower case the characters in the string\n",
    "    text = text.lower()\n",
    "    \n",
    "    # REMOVAL OF EXTRA WHITESPACES\n",
    "    # remove duplicated spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    # remove leading and trailing spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "894XvPzNb7vH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LVX2z55b_kv"
   },
   "source": [
    "The following fucntion passes a sentence that has been entered by the user to the model and returns its answer along with the input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "0rUgD4MecAdz"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    \"\"\"\n",
    "    This function takes a sentence (question) and returns the model's output\n",
    "    (answer) to it.\n",
    "    Arguemnts:\n",
    "      sentence: a string.\n",
    "    Returns:\n",
    "      result: a string, representing model's output to the input.\n",
    "      sentence: a string, the input sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    # clean the input sentence (question) to prepare for the encoder\n",
    "    sentence = clean_text(sentence)\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "\n",
    "    # tokenize the input sentence and pad zeros if its length is less than\n",
    "    # maximum sequence length.\n",
    "    inputs = [text_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  \n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    # initilize the hidden state of the encoder\n",
    "    enc_hidden = (tf.zeros((1, units)), \n",
    "                  tf.zeros((1, units)), \n",
    "                  tf.zeros((1, units)), \n",
    "                  tf.zeros((1, units)))\n",
    "    \n",
    "    enc_output, enc_hidden, enc_c = encoder(inputs, enc_hidden)\n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([text_tokenizer.word_index['<start>']], 0)\n",
    "    \n",
    "    # generate answer, where the maximum length for the answer is equal\n",
    "    # to max_length_targ=32\n",
    "    for t in range(max_length_targ):\n",
    "      predictions, dec_hidden, attention_weights = decoder(dec_input, \n",
    "                                                          dec_hidden, \n",
    "                                                          enc_output)\n",
    "\n",
    "      predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "      result += text_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "      if text_tokenizer.index_word[predicted_id] == '<end>':\n",
    "        return result, sentence\n",
    "\n",
    "      # the predicted ID is fed back into the model\n",
    "      dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "swhPdcLhcNuV"
   },
   "outputs": [],
   "source": [
    "def answer(sentence):\n",
    "  \"\"\"\n",
    "  This function takes an input sentence by the user and prints the \n",
    "  model's answer along with the user's input sentence.\n",
    "  Arguments:\n",
    "      sentence: a string.\n",
    "  \"\"\"\n",
    "\n",
    "  result, sentence = evaluate(sentence)\n",
    "\n",
    "  print(f'INPUT: {sentence}')\n",
    "  print(f'CHATBOT ANSWER: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52LmYigxcYo4"
   },
   "source": [
    "Let's try to talk to the chatbot:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YHPv7nbKcZo3",
    "outputId": "b585d000-ad25-4928-8af5-982bd63f4e37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> tell me something funny <end>\n",
      "CHATBOT ANSWER: nan <end> \n"
     ]
    }
   ],
   "source": [
    "answer('tell me something funny')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rby0I7kGcddX",
    "outputId": "d0b826e2-2e95-414e-bcfa-d0f79231e5df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> what is it that you want in life ? <end>\n",
      "CHATBOT ANSWER: me : well , i ' m not strange . <end> \n"
     ]
    }
   ],
   "source": [
    "answer('What is it that you want in life?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GvLfMiJJcpMZ",
    "outputId": "1743f1b7-0d52-4c82-cf9e-d1c12df58a64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> you ' re so fun <end>\n",
      "CHATBOT ANSWER: you know what you ' re a . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"you're so fun\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXKLi3h4c1tK",
    "outputId": "c74e8c8b-eef0-4ae3-e626-8e10f5a372bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> do you like annoying people ? <end>\n",
      "CHATBOT ANSWER: well , i ' ll get jalapeno business . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"do you like annoying people?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X4yq_0_Yc8aT",
    "outputId": "6f569dd2-83cd-465e-9b68-d060f812c43a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> are you my friend ? <end>\n",
      "CHATBOT ANSWER: because you ' re giving me a . <end> \n"
     ]
    }
   ],
   "source": [
    "answer('are you my friend?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7FJcHyoidBsj",
    "outputId": "4298df2f-4cd1-4cb7-e576-090b01a3f90a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> what is your favorite movie ? <end>\n",
      "CHATBOT ANSWER: me : i ' m not sure . ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' \n"
     ]
    }
   ],
   "source": [
    "answer(\"what is your favorite movie?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pd7uqq-hdJ5w",
    "outputId": "47b31b8b-7123-4f36-c96d-7cdae15328e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> how do you sleep at night ? <end>\n",
      "CHATBOT ANSWER: by my wife . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"how do you sleep at night?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-I4jNQTadVpr",
    "outputId": "5fe72c07-81d9-4a71-dc24-eab23e35bd67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> what will happen if you went inside a black hole ? <end>\n",
      "CHATBOT ANSWER: i don ' t know what you get a pikachu ! <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"what will happen if you went inside a black hole?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKgVQ-JzdbDW",
    "outputId": "d74d3358-54b1-43c8-ae29-402d1115fa1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> if we count sheep to fall asleep , what do they count ? <end>\n",
      "CHATBOT ANSWER: a . <end> \n"
     ]
    }
   ],
   "source": [
    "answer('If we count sheep to fall asleep, what do they count?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7U1bSBMdfo7",
    "outputId": "bd30deb7-a974-414a-d543-8ab3023add6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> why do people order double cheese burgers , large fries , and a diet coke ? <end>\n",
      "CHATBOT ANSWER: because they are hungry . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"Why do people order double cheese burgers, large fries, and a diet coke?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wzBCs138di4P",
    "outputId": "2b604127-ba33-4510-9675-a469705ba97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> what is always coming , but never arrives ? <end>\n",
      "CHATBOT ANSWER: what do you call a man who is a man . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"What is always coming, but never arrives?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "357po0YGdodB",
    "outputId": "0d7be9cf-371b-45b7-beeb-6ad4af8b4eac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> who is the most stupid person you know ? <end>\n",
      "CHATBOT ANSWER: nobody . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"who is the most stupid person you know?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RQTsCVQvdvhe",
    "outputId": "575b3963-2f8b-4890-c252-6cdd4ad12976"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> knock knock <end>\n",
      "CHATBOT ANSWER: knock knock . \n",
      "\n",
      "who ' s there ? who ? - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "answer(\"knock knock\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tPHLRFR8dzuC",
    "outputId": "b5fcbb92-b90b-4235-be32-373c35749b47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> tell me a joke <end>\n",
      "CHATBOT ANSWER: a what , you ' re a comedian ? <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"tell me a joke\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AnUZiEDId4M9",
    "outputId": "872e426a-ae20-4b88-f85a-3ad649b82081"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> do you exercise ? <end>\n",
      "CHATBOT ANSWER: no , but i ' m not sure i ' m not sure i ' m not sure i ' m not sure i ' m not sure i ' m not \n"
     ]
    }
   ],
   "source": [
    "answer(\"Do you exercise?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UlXtKR0Bd97l",
    "outputId": "d4212f32-8307-43a9-f7bc-4a180fdb6367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> how do you keep a clear mind during hard times ? <end>\n",
      "CHATBOT ANSWER: i ' ll tell you tomorrow . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"How do you keep a clear mind during hard times?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y2XIZ-6GeBoY",
    "outputId": "3655d723-cef6-4a2f-a4d9-2c1fdd278440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> who is your best friend ? <end>\n",
      "CHATBOT ANSWER: and why is your boss on your ass . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"who is your best friend?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ZdoEhNDeH66",
    "outputId": "d0989143-6476-4086-f849-c96862a8aeb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> what do you have in mind ? <end>\n",
      "CHATBOT ANSWER: a real treat . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"what do you have in mind?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8l3sC72erSy",
    "outputId": "198e2111-96f1-4f10-c0a8-7bf99a5812ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> who is you favorite singer ? <end>\n",
      "CHATBOT ANSWER: dr . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"who is you favorite singer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7e9jnWXesNt",
    "outputId": "217d0043-abb8-41f0-d8d9-d3b2f3c52096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> are you enjoying this conversation ? <end>\n",
      "CHATBOT ANSWER: because you look like a 10 / 10 . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"Are you enjoying this conversation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDsVxFvvezBd",
    "outputId": "be920464-7ebd-484a-ca4e-f2d79f5e6f99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> do you know google assistant ? <end>\n",
      "CHATBOT ANSWER: i bet you are polar <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"Do you know Google assistant?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xmxp7rWtez0x",
    "outputId": "ce5e2d38-18cd-4567-b81b-763c73e25134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> what do you like to talk about the most ? <end>\n",
      "CHATBOT ANSWER: i don ' t know . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"what do you like to talk about the most?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOk9yvMpe4U4",
    "outputId": "cc736de6-239a-4a7b-cfff-e1a002ff50b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> why do you like to talk about fire ? <end>\n",
      "CHATBOT ANSWER: you can ' t get it <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"Why do you like to talk about fire?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1-KgZao1e7ef",
    "outputId": "4485d1cf-5b09-4f67-a437-e171ae67e94f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> what was the best thing before sliced bread ? <end>\n",
      "CHATBOT ANSWER: massive sandwiches <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"What was the best thing before sliced bread?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SBf7j1ZcfABd",
    "outputId": "8df808db-a245-479a-eb0d-ea3f75388b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> what is better than the fountain of youth ? <end>\n",
      "CHATBOT ANSWER: a dead people <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"What is better than the FOUNTAIN OF YOUTH?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HYqw_nKffDEx",
    "outputId": "be2686d0-e4cb-4e36-f985-7ec60ca5e8dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> are you lazy ? <end>\n",
      "CHATBOT ANSWER: because i ' m not a ho ! <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"Are you lazy?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9-g_wrMfHFr",
    "outputId": "f3696226-b344-46dd-8ecd-c76918c6f587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> how do you know that you ' ll never quit smoking ? <end>\n",
      "CHATBOT ANSWER: because when you ' re all . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"How do you know that you'll never quit smoking?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71qmGXp4fK7w",
    "outputId": "f6b7b300-562a-4b63-eae2-26e55329939f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> when will you quit smoking ? <end>\n",
      "CHATBOT ANSWER: when your hair smells nice . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"When will you quit smoking?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eI4DbQ5ifOBQ",
    "outputId": "45dd8e0e-95a9-4c30-ed76-b5edb1f6ec40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> who talks the most ? <end>\n",
      "CHATBOT ANSWER: mr . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"Who talks the most?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JLJVZAOfSAS",
    "outputId": "d61901bb-c49d-4df1-83f1-a7b357b0171f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> who lies the most ? <end>\n",
      "CHATBOT ANSWER: sir , let me tell me by the night . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"Who lies the most?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tH3RWYXJfU1l",
    "outputId": "3fa722f2-50fb-4b92-e51c-553584f24e7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> what do you know about computers ? <end>\n",
      "CHATBOT ANSWER: it ' s a little things <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"What do you know about computers?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GuY4cToafXkO",
    "outputId": "652cca3d-9ac2-44b2-de29-9f10e0d864a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> where do like to travel ? <end>\n",
      "CHATBOT ANSWER: the . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"Where do like to travel?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XqkE9IvgfaXa",
    "outputId": "ca22663b-a337-494e-df7f-b76e4f6fbf88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> which country do you want to visit ? <end>\n",
      "CHATBOT ANSWER: . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"Which country do you want to visit?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AlA0Jhk_ffD9",
    "outputId": "7b1a6a8d-5647-4260-bdae-b59d43a48a30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> which type of music do you like ? <end>\n",
      "CHATBOT ANSWER: - old : the answer says , ' i ' m a big metal fan . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"Which type of music do you like?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ho8dd2ckfh87",
    "outputId": "3a71e416-ad6d-476f-d542-745a61f0f77b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> what ' s your job ? <end>\n",
      "CHATBOT ANSWER: me : * sweating * licks lips * <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"what's your job?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kl9L42RGflEM",
    "outputId": "1945a6e9-38f2-4949-e6ff-040901233612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> are you an artificial intelligence model ? <end>\n",
      "CHATBOT ANSWER: you are my own . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"Are you an artificial intelligence model?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKjJ8Qipfo-_",
    "outputId": "2056fc6d-506a-451c-ece7-2df98aec1fe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> how are you doing ? <end>\n",
      "CHATBOT ANSWER: i ' m not sure , ' i ' m not wearing sunglasses emoji . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"How are you doing?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9LkU3mrSfsmJ",
    "outputId": "6bb7a160-4c6c-408c-b71a-2f6bc419263c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> how are you ? <end>\n",
      "CHATBOT ANSWER: i ' m not a ' ' <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"How are you?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvPR-3dFfvn_",
    "outputId": "ca24c264-2c2f-470c-9453-3d1c86bf4d92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> how was your day ? <end>\n",
      "CHATBOT ANSWER: i said , ' oh , i ' m not sure , ' i replied ' i ' m not sure , ' i replied ' i ' m not sure , \n"
     ]
    }
   ],
   "source": [
    "answer(\"How was your day?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6ASX8sWfylt",
    "outputId": "06bb2c79-870e-4055-9dd0-6491fa6c29eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> where do you vacation ? <end>\n",
      "CHATBOT ANSWER: in a <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"Where do you vacation?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aLwCBVkEf3DG",
    "outputId": "42ab383d-9472-4ecc-a1df-235488a11a08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> thanks for your time . <end>\n",
      "CHATBOT ANSWER: you ' re a pain in your back . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"Thanks for your time.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MlpG-isPf6Ml",
    "outputId": "0d86d9e4-f1f8-459f-9228-e558893a73c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> what is your least favorite food ? <end>\n",
      "CHATBOT ANSWER: waiter : it ' s <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"What is your least favorite food?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EktR9hzif-O8",
    "outputId": "d2e685fd-38fd-4e3b-9356-c9d6ade78392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> what is the most hilarious childhood memory you can think of ? <end>\n",
      "CHATBOT ANSWER: <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"What is the most hilarious childhood memory you can think of?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UtWvSlizgBPg",
    "outputId": "155c9a57-084e-429a-c15b-05480b922d93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> are you funny ? <end>\n",
      "CHATBOT ANSWER: because you ' re a . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"Are you funny?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S7dfNIeDgFWm",
    "outputId": "881c7f46-597c-4892-eebd-0e7ac44172d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> beethoven or bach ? <end>\n",
      "CHATBOT ANSWER: why did beethoven kill me : ' i ' ve never be bach , bach , i ' ve never know you , he was a seasoned veteran . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"Beethoven or Bach?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JktyN4x2gIMN",
    "outputId": "8285c3b4-82e6-4433-ffb3-fa05ef851f99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> how do you like your coffee ? <end>\n",
      "CHATBOT ANSWER: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n"
     ]
    }
   ],
   "source": [
    "answer(\"How do you like your coffee?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0fw08VXgMSL",
    "outputId": "fbccb4f7-937c-4fcd-89bd-f559a0069218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> what is your favorite sport ? <end>\n",
      "CHATBOT ANSWER: 7 - old . <end> \n"
     ]
    }
   ],
   "source": [
    "answer(\"What is your favorite sport?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FF9GVoUNgQJ9",
    "outputId": "8215f0b7-486e-4b26-86b0-441d91742f82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: <start> what is your favorite car ? <end>\n",
      "CHATBOT ANSWER: chihuahua : i ' m not sure , but i ' m not sure , but i ' m not sure , but i ' m not sure , but i ' \n"
     ]
    }
   ],
   "source": [
    "answer(\"What is your favorite car?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "BrwjZ7qEgT7H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWm7WQsygXmA"
   },
   "source": [
    "Start a conversation with the chatbot:\n",
    "\n",
    "The following function takes an input sentence from the user and prints the model's answer to it until the user enters exit() to finish the conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "8WZs-gXkgYl4"
   },
   "outputs": [],
   "source": [
    "def chat():\n",
    "    \"\"\"\n",
    "    This function takes input from the user and outputs the chatbot's answer\n",
    "    until the user inputs `exit()`\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Start your conversation with the chatbot!\")\n",
    "    print(\"If you want to end this conversation, enter: exit()\\nHave fun!\")  \n",
    "\n",
    "    while True:\n",
    "      # take user input\n",
    "      user_input = str(input('>>'))\n",
    "      \n",
    "      if user_input == 'exit()':\n",
    "        break\n",
    "\n",
    "      try:\n",
    "        # generate an answer for the user's question.\n",
    "        chatbot_answer, _ = evaluate(user_input)\n",
    "        print(chatbot_answer)\n",
    "      except:\n",
    "        # in case there's word out of chatbot vocabulary.\n",
    "        print(\"Oops! can't help you there! Try different words or restructure your sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ovK5NNJgk9m",
    "outputId": "ff0eb0ed-0cff-4ac6-f399-492df74302f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start your conversation with the chatbot!\n",
      "If you want to end this conversation, enter: exit()\n",
      "Have fun!\n",
      ">>hi\n",
      "a : and a mute are in a lightbulb . <end> \n",
      ">>what ' s the best anti diarrheal prescription ?\n",
      "Oops! can't help you there! Try different words or restructure your sentence.\n",
      ">>what do you call a russian tree ?\n",
      "<end> \n",
      ">>how do you call it when an egg is on point\n",
      "egg . <end> \n",
      ">>why are frogs so happy ?\n",
      "because they eat whatever bugs them . <end> \n",
      ">>what is a pirate ' s worst nightmare ?\n",
      "a sunken chest and no booty . <end> \n",
      ">>why does snoopdogg carry an umbrella ?\n",
      "Oops! can't help you there! Try different words or restructure your sentence.\n",
      ">>why can ' t obama poke fun at himself ?\n",
      "because that would be racist . <end> \n",
      ">>why do turkeys always gobble ?\n",
      "they ' re full . <end> \n",
      ">>why google is a she ?\n",
      "she can ' t let you finish your mind . <end> \n",
      ">>why shouldn ' t you fart in church ?\n",
      "because they don ' t have to sit in the oven . <end> \n",
      ">>what ' s green and has 4 wheels ?\n",
      "grass , i lied about the wheels . <end> \n",
      ">>what do you call water that bounces ?\n",
      "spring water . <end> \n",
      ">>exit()\n"
     ]
    }
   ],
   "source": [
    "chat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nUclib2hlTy"
   },
   "source": [
    "# Conclusion\r\n",
    "As we notice, not all the answers generated by the model are good or valid answers and there's a lot to improve here. For example, there's some answers with repeated blocks of words. Also, some of the answers are not valid responses to the questions and do not relate the topic being proposed in the question. This kind of performance is expected for a deep learning chatbot with this very small dataset but it's okay for a demo project which can be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "gP1oXOSpgokr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOnirwdPhsSH"
   },
   "source": [
    "# Next\n",
    "## What can we do to improve performance:\n",
    "**More data**: if we want to have good performance with somewhat impressive responses, we need to train on a lot more data, maybe millions of pairs. However, training on a large scale dataset requires more computational power (maybe a more powerful GPU) and a more sophisticated model with more layers and units, let alone finding and acquiring such dataset.\n",
    "\n",
    "**DISCLAIMER**: the model has been trained on 175,671 question-answer pairs which have been gathered from different sources like reddit, and it turns out that some of the model's answers are impolite, just something to be aware of.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Further Reading:\n",
    " - Effective Approaches to Attention-based Neural Machine Translation\n",
    "\n",
    "  - Neural machine translation with attention\n",
    "\n",
    "  - Neural Machine Translation by Jointly Learning to Align and Translate\n",
    "\n",
    "   - Neural Machine Translation (seq2seq) Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "y8xtTTOSiKfI"
   },
   "outputs": [],
   "source": [
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "f28Ce-DuotAv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Smart_bot.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
